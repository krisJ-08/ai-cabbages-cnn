{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0068379",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:45.088520Z",
     "iopub.status.busy": "2022-07-18T14:50:45.087870Z",
     "iopub.status.idle": "2022-07-18T14:50:52.089998Z",
     "shell.execute_reply": "2022-07-18T14:50:52.089044Z"
    },
    "papermill": {
     "duration": 7.010313,
     "end_time": "2022-07-18T14:50:52.092671",
     "exception": false,
     "start_time": "2022-07-18T14:50:45.082358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2ac7b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:52.101366Z",
     "iopub.status.busy": "2022-07-18T14:50:52.100821Z",
     "iopub.status.idle": "2022-07-18T14:50:52.106193Z",
     "shell.execute_reply": "2022-07-18T14:50:52.105366Z"
    },
    "papermill": {
     "duration": 0.011793,
     "end_time": "2022-07-18T14:50:52.108159",
     "exception": false,
     "start_time": "2022-07-18T14:50:52.096366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the directories \n",
    "training_dir = '../input/cabbagetypescnn/cabbage'\n",
    "#validation_dir = '../input/fruits/fruits-360_dataset/fruits-360/Test/'\n",
    "#test_dir = '../input/fruits/fruits-360_dataset/fruits-360/test-multiple_fruits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261dd190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:52.117168Z",
     "iopub.status.busy": "2022-07-18T14:50:52.115782Z",
     "iopub.status.idle": "2022-07-18T14:50:52.143382Z",
     "shell.execute_reply": "2022-07-18T14:50:52.142490Z"
    },
    "papermill": {
     "duration": 0.034007,
     "end_time": "2022-07-18T14:50:52.145402",
     "exception": false,
     "start_time": "2022-07-18T14:50:52.111395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# useful for getting number of files\n",
    "image_files = glob(training_dir + '/*/*.jp*g')\n",
    "#valid_image_files = glob(validation_dir + '/*/*.jp*g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1b46c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:52.153491Z",
     "iopub.status.busy": "2022-07-18T14:50:52.152951Z",
     "iopub.status.idle": "2022-07-18T14:50:52.158549Z",
     "shell.execute_reply": "2022-07-18T14:50:52.157550Z"
    },
    "papermill": {
     "duration": 0.014117,
     "end_time": "2022-07-18T14:50:52.162926",
     "exception": false,
     "start_time": "2022-07-18T14:50:52.148809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Classes = 4\n"
     ]
    }
   ],
   "source": [
    "# getting the number of classes i.e. type of fruits\n",
    "folders = glob(training_dir + '/*')\n",
    "num_classes = len(folders)\n",
    "print ('Total Classes = ' + str(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764b3c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:52.171789Z",
     "iopub.status.busy": "2022-07-18T14:50:52.171542Z",
     "iopub.status.idle": "2022-07-18T14:50:52.175819Z",
     "shell.execute_reply": "2022-07-18T14:50:52.174835Z"
    },
    "papermill": {
     "duration": 0.010746,
     "end_time": "2022-07-18T14:50:52.177994",
     "exception": false,
     "start_time": "2022-07-18T14:50:52.167248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_df, validate_df = train_test_split(training_dir, test_size=0.20, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604c0055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:52.186765Z",
     "iopub.status.busy": "2022-07-18T14:50:52.185989Z",
     "iopub.status.idle": "2022-07-18T14:50:56.080337Z",
     "shell.execute_reply": "2022-07-18T14:50:56.079371Z"
    },
    "papermill": {
     "duration": 3.901268,
     "end_time": "2022-07-18T14:50:56.082749",
     "exception": false,
     "start_time": "2022-07-18T14:50:52.181481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:50:52.297071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:52.404698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:52.405494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:52.406739: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-18 14:50:52.407057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:52.407738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:52.408397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:54.630103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:54.630961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:54.631647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 14:50:54.632261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "#from keras.preprocessing import image\n",
    "\n",
    "IMAGE_SIZE = [150, 150]  # we will keep the image size as (64,64). You can increase the size for better results. \n",
    "\n",
    "# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n",
    "vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n",
    "\n",
    "# this will exclude the initial layers from training phase as there are already been trained.\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg.output)\n",
    "#x = Dense(128, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\n",
    "x = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
    "\n",
    "model = Model(inputs = vgg.input, outputs = x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "120194df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:56.097148Z",
     "iopub.status.busy": "2022-07-18T14:50:56.096753Z",
     "iopub.status.idle": "2022-07-18T14:50:56.104013Z",
     "shell.execute_reply": "2022-07-18T14:50:56.103057Z"
    },
    "papermill": {
     "duration": 0.01626,
     "end_time": "2022-07-18T14:50:56.106961",
     "exception": false,
     "start_time": "2022-07-18T14:50:56.090701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 32772     \n",
      "=================================================================\n",
      "Total params: 14,747,460\n",
      "Trainable params: 32,772\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f022ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:56.117137Z",
     "iopub.status.busy": "2022-07-18T14:50:56.116856Z",
     "iopub.status.idle": "2022-07-18T14:50:56.328070Z",
     "shell.execute_reply": "2022-07-18T14:50:56.327237Z"
    },
    "papermill": {
     "duration": 0.218564,
     "end_time": "2022-07-18T14:50:56.329992",
     "exception": false,
     "start_time": "2022-07-18T14:50:56.111428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82 images belonging to 4 classes.\n",
      "Found 20 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=200,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    training_dir, # same directory as training data\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=200,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "#model.fit_generator(\n",
    "#    train_generator,\n",
    "#    steps_per_epoch = train_generator.samples // 200,\n",
    "#    validation_data = validation_generator, \n",
    "#    validation_steps = validation_generator.samples // 200,\n",
    " #   epochs = nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc52aac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:56.340378Z",
     "iopub.status.busy": "2022-07-18T14:50:56.339732Z",
     "iopub.status.idle": "2022-07-18T14:50:56.344370Z",
     "shell.execute_reply": "2022-07-18T14:50:56.343332Z"
    },
    "papermill": {
     "duration": 0.012056,
     "end_time": "2022-07-18T14:50:56.346714",
     "exception": false,
     "start_time": "2022-07-18T14:50:56.334658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image Augmentation\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#training_datagen = ImageDataGenerator(\n",
    " #                                   rescale=1./255,   # all pixel values will be between 0 an 1\n",
    " #                                   shear_range=0.2, \n",
    "  #                                  zoom_range=0.2,\n",
    "  #                                  horizontal_flip=True,\n",
    "   #                                 preprocessing_function=preprocess_input)\n",
    "\n",
    "#validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "#training_generator = training_datagen.flow_from_directory(train_df, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')\n",
    "#validation_generator = validation_datagen.flow_from_directory(validate_df, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c27536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:56.356394Z",
     "iopub.status.busy": "2022-07-18T14:50:56.356155Z",
     "iopub.status.idle": "2022-07-18T14:50:56.364016Z",
     "shell.execute_reply": "2022-07-18T14:50:56.363071Z"
    },
    "papermill": {
     "duration": 0.015127,
     "end_time": "2022-07-18T14:50:56.366076",
     "exception": false,
     "start_time": "2022-07-18T14:50:56.350949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bokchoy': 0, 'napa': 1, 'red': 2, 'savoy': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels are stored in class_indices in dictionary form. \n",
    "# checking the labels\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15b541b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:50:56.377826Z",
     "iopub.status.busy": "2022-07-18T14:50:56.376319Z",
     "iopub.status.idle": "2022-07-18T14:51:32.650480Z",
     "shell.execute_reply": "2022-07-18T14:51:32.649428Z"
    },
    "papermill": {
     "duration": 36.2825,
     "end_time": "2022-07-18T14:51:32.653383",
     "exception": false,
     "start_time": "2022-07-18T14:50:56.370883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "2022-07-18 14:50:59.047495: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:51:02.250719: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 14s 14s/step - loss: 1.4402 - accuracy: 0.3780 - val_loss: 1.2190 - val_accuracy: 0.4500\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.2439 - accuracy: 0.3659 - val_loss: 0.9797 - val_accuracy: 0.6500\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0342 - accuracy: 0.5610 - val_loss: 0.8375 - val_accuracy: 0.6000\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8414 - accuracy: 0.7195 - val_loss: 0.7485 - val_accuracy: 0.8000\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6878 - accuracy: 0.8049 - val_loss: 0.6609 - val_accuracy: 0.8500\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5863 - accuracy: 0.8902 - val_loss: 0.5541 - val_accuracy: 0.9000\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4911 - accuracy: 0.9146 - val_loss: 0.5570 - val_accuracy: 0.9000\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4064 - accuracy: 0.9634 - val_loss: 0.4464 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "#training_images = 82\n",
    "#validation_images = 20\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "  #                 steps_per_epoch = 82,  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n",
    "                   epochs = 8,  # change this for better results\n",
    "                   validation_data = validation_generator)\n",
    "      #             validation_steps = 20)  # this should be equal to total number of images in validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "847efd10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T14:51:32.667526Z",
     "iopub.status.busy": "2022-07-18T14:51:32.667237Z",
     "iopub.status.idle": "2022-07-18T14:51:32.673212Z",
     "shell.execute_reply": "2022-07-18T14:51:32.672269Z"
    },
    "papermill": {
     "duration": 0.0159,
     "end_time": "2022-07-18T14:51:32.675783",
     "exception": false,
     "start_time": "2022-07-18T14:51:32.659883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = [0.37804877758026123, 0.3658536672592163, 0.5609756112098694, 0.7195122241973877, 0.8048780560493469, 0.8902438879013062, 0.9146341681480408, 0.9634146094322205]\n",
      "Validation Accuracy = [0.44999998807907104, 0.6499999761581421, 0.6000000238418579, 0.800000011920929, 0.8500000238418579, 0.8999999761581421, 0.8999999761581421, 0.949999988079071]\n"
     ]
    }
   ],
   "source": [
    "print ('Training Accuracy = ' + str(history.history['accuracy']))\n",
    "print ('Validation Accuracy = ' + str(history.history['val_accuracy']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 59.395195,
   "end_time": "2022-07-18T14:51:36.233917",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-18T14:50:36.838722",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
